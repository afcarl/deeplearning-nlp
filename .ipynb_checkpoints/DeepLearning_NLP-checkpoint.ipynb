{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window based coocurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Running PCA using SVD produces _okay_ results.\n",
    "+ `word2vec` works by predicting surrounding words instead of capturing coocurrence directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/personal-notes/word2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentialy \"dynamic\" logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GloVe` (Global Vectors) provides key benefits:\n",
    "+ Fast training\n",
    "+ Scalalble to big corpora\n",
    "+ Good perf even with small corpus/vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Vector Analogies work by how well a given word's cosine distance after additon captures intuitive semantic and syntactic analogy questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Tie the weights at each time step and then condition the NN on all previous words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/personal-notes/rnn_langmodel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity: performance likelihood of predicting the next word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU provides more complex hidden unit computation in the recurrence. GRU is a special case of LSTM\n",
    "\n",
    "_Main idea_:\n",
    "\n",
    "+ Keep around memories to capture long distance dependencies\n",
    "+ Allow error messages to flow at different strengths depending on the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to existing problems: look up Merity et al. 2016 (Pointer Sentinel Mixture Models)\n",
    "\n",
    "+ Combine softmax with pointers to context words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Memory Networks\n",
    "\n",
    "#### Obstacle 1:\n",
    "\n",
    "_No single NLP model architecture gets consistently state-of-the-art results across tasks_\n",
    "\n",
    "|Task|State of the art model|\n",
    "|-|-|\n",
    "|Question answering (babl)|Strongly Supervised MemNN|\n",
    "|Sentiment Analysis (SST)|Tree-LSTMs|\n",
    "|Part of speech tagging (PTB-WSJ)|Bi-directional LSTM-CRF|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obstacle 2:\n",
    "\n",
    "Fully joint multitask learning (where same decoder/classifier and not only transfer learning) is _hard_\n",
    "\n",
    "Usually:\n",
    "\n",
    "+ Restricted to lower layers\n",
    "+ Helps only if tasks are related\n",
    "+ Often hurts performance if tasks are not related..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model example:\n",
    "\n",
    "![](https://storage.googleapis.com/personal-notes/dynamic_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple passes demonstrably show improvement:\n",
    "\n",
    "![](https://storage.googleapis.com/personal-notes/dmn_sentiment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DMN modularity allows for a swap of the input module, such as for CV:\n",
    "+ Visual feature extraction through a CNN is passed to feature-embedding vectors and then sent to the input fusion layer GRU of the DMN\n",
    "+ This model is `VQA: Visual Question Answering`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "![](https://storage.googleapis.com/personal-notes/attention_viz.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
